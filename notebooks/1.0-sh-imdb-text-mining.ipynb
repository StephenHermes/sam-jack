{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML: Movie Learning\n",
    "\n",
    "We're going to do some form of movie text wrangling using Python. First, we need to collect some data.\n",
    "\n",
    "## 1. Data Collection\n",
    "\n",
    "### 1.1. The OMDb API\n",
    "\n",
    "We're going to use the OMDb API, which is free once again (http://www.omdbapi.com/). To use it, you need to get an API key. You can get a free key, which limits to 1000 requests per day. More requests (and a poster API) are available if you patronize the OMDb Patreon. The OMDb website explains how the API works pretty well. We'll use the `requests` package to make calls to the OMDb API. \n",
    "\n",
    "There are two ways we can get movie data using this API, either by movie title or IMDb ID. We'll want to be able to handle either, as I have a feeling that it will be easier to get a random list of IMDb ID's than a random list of movie titles? It is also more exact to use ID's since movie titles aren't unique (e.g.,\"The Mummy\" can either refer to the Brendan Fraser masterpiece, or the Tom Cruise dumpster fire).\n",
    "\n",
    "Note, I'm using format strings (`f'some {text}'`) which is a Python 3.6 feature (equivalent to `'some {}.format(text)'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import requests\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "#find .env automagically by walking up directories until it's found\n",
    "dotenv_path = find_dotenv()\n",
    "# load up the entries as environment variables\n",
    "load_dotenv(dotenv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = os.environ.get('OMDB_API_KEY')\n",
    "\n",
    "def get_movie_data(name, year=None, api_key=API_KEY, full_plot=False):\n",
    "    \"\"\"Returns json from OMDb API for movie.\"\"\"\n",
    "    \n",
    "    api_url = f'http://www.omdbapi.com/?apikey={api_key}'\n",
    "    # There are actually utilities that can automatically escape invalid characters\n",
    "    # but here we do the manual dumb solution\n",
    "    name = name.lower().replace(' ', '+')\n",
    "    \n",
    "    # Can either manually extend the url with parameters or...\n",
    "    #api_url += f'&t={name}'\n",
    "    # if year is not None:\n",
    "    #     api_url += f'&y={year}'\n",
    "    # if full_plot:\n",
    "    #     api_url += '&plot=full'\n",
    "    # response = requests.get(api_url)\n",
    "    \n",
    "    # ... have `requests` do it for you!\n",
    "    body = {'t': name}\n",
    "    if year is not None:\n",
    "        body['y'] = year\n",
    "    if full_plot:\n",
    "        body['plot'] = 'full'\n",
    "    response = requests.get(api_url, params=body)\n",
    "    \n",
    "    # Throw error if API call has an error\n",
    "    if response.status_code != 200:\n",
    "        raise requests.HTTPError(\n",
    "            f'Couldn\\'t call API. Error {response.status_code}.'\n",
    "        )\n",
    "     \n",
    "    # Throw error if movie not found\n",
    "    if response.json()['Response'] == 'False':\n",
    "        raise ValueError(response.json()['Error'])\n",
    "    \n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it out, and see what kind of information we get from our request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Title\": \"Snakes on a Plane\",\n",
      "    \"Year\": \"2006\",\n",
      "    \"Rated\": \"R\",\n",
      "    \"Released\": \"18 Aug 2006\",\n",
      "    \"Runtime\": \"105 min\",\n",
      "    \"Genre\": \"Action, Adventure, Crime\",\n",
      "    \"Director\": \"David R. Ellis\",\n",
      "    \"Writer\": \"John Heffernan (screenplay), Sebastian Gutierrez (screenplay), David Dalessandro (story), John Heffernan (story)\",\n",
      "    \"Actors\": \"Samuel L. Jackson, Julianna Margulies, Nathan Phillips, Rachel Blanchard\",\n",
      "    \"Plot\": \"An F.B.I. Agent takes on a plane full of deadly venomous snakes, deliberately released to kill a witness being flown from Honolulu to Los Angeles to testify against a mob boss.\",\n",
      "    \"Language\": \"English\",\n",
      "    \"Country\": \"Germany, USA, Canada\",\n",
      "    \"Awards\": \"3 wins & 7 nominations.\",\n",
      "    \"Poster\": \"https://m.media-amazon.com/images/M/MV5BZDY3ODM2YTgtYTU5NC00MTE4LTkzNjktMzNhZWZmMzJjMWRjXkEyXkFqcGdeQXVyMTQxNzMzNDI@._V1_SX300.jpg\",\n",
      "    \"Ratings\": [\n",
      "        {\n",
      "            \"Source\": \"Internet Movie Database\",\n",
      "            \"Value\": \"5.5/10\"\n",
      "        },\n",
      "        {\n",
      "            \"Source\": \"Rotten Tomatoes\",\n",
      "            \"Value\": \"69%\"\n",
      "        },\n",
      "        {\n",
      "            \"Source\": \"Metacritic\",\n",
      "            \"Value\": \"58/100\"\n",
      "        }\n",
      "    ],\n",
      "    \"Metascore\": \"58\",\n",
      "    \"imdbRating\": \"5.5\",\n",
      "    \"imdbVotes\": \"123,683\",\n",
      "    \"imdbID\": \"tt0417148\",\n",
      "    \"Type\": \"movie\",\n",
      "    \"DVD\": \"02 Jan 2007\",\n",
      "    \"BoxOffice\": \"$33,886,034\",\n",
      "    \"Production\": \"New Line Cinema\",\n",
      "    \"Website\": \"http://www.snakesonaplane.com/\",\n",
      "    \"Response\": \"True\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response_json = get_movie_data('Snakes on a Plane')\n",
    "print(json.dumps(response_json, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are all sorts of fun things we could do with this data, such as:\n",
    "1. Predict something based off of the description, like the Rotten Tomatoes score (or just certified fresh or rotten), or the genre, or if Samuel L. Jackson is in the movie or not.\n",
    "2. Train a sentiment analysis algorithm using the score as a proxy for positivity.\n",
    "3. Predict the BoxOffice based off of engineered features, like the cast, genre, whether or not Sam Jack is in it...\n",
    "4. Generate a short synopsis based off of the movie poster.\n",
    "\n",
    "Genre classification might be a fun one, but is well-tread territory at this point. Maybe we'll train a classifier to determine based off of a movie pitch (aka plot summary) whether or not Samuel L. Jackson would be in it. This seems like a nice balance between interesting, and easy to do.\n",
    "\n",
    "Note there is a `Response` field in the response, which is `\"True\"` if the movie is found, and `\"False\"` if not. At the moment I feel like it would be better to catch this early on in the process, so we added a few lines to throw an Exception when the movie is not found in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception: Movie not found!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    get_movie_data('Snakes on a Plan')\n",
    "except Exception as e:\n",
    "    print('Exception:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now we just need a giant list of either movie names or IMDB ids..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.2. Getting a List of Movies\n",
    "\n",
    "Using `requests` we now have a way of extracting data about a film, given its title. Now all we need is a list of movies over which to iterate. After a bit of googling, I found a Wikipedia page listing movies, but it is organized through several subpages. We'll use the BeautifulSoup package to crawl these pages to generate our list of movies.\n",
    "\n",
    "There are two paths of attack:\n",
    "1. `https://en.wikipedia.org/wiki/Lists_of_films` has data on every movie ever made. There are a few different hierarchies movies are classified by on this. This catalogs every movie ever made.\n",
    "2. `https://en.wikipedia.org/wiki/Lists_of_American_films` is just American films, organized by year. This is maybe easier to work with, since the organization is more orderly.\n",
    "\n",
    "Since this project is ultimately stupid, I'll just do the easier thing and use the American movie list. Additionally, the American film list is split up by year, where the different years have their own pages. The urls for these pages are very consistent, so easy to crawl. Each year's page has the movies listed in tables, which appear to always have the same columns.\n",
    "\n",
    "For this part I used this tutorial https://goo.gl/Bm1cdD (sort of).\n",
    "\n",
    "A web page is basically a tree with nodes labeled by tags. BeautifulSoup packages this tree in an object that is easy to navigate and search, based off of the html tags attached to the nodes. Here's how we are going to pull the movie data for any given year.\n",
    "1. We pull the raw html using requests and passing the url to a `GET` request.\n",
    "2. Dump the html string into a BeautifulSoup object.\n",
    "3. From inspecting the raw html it looks like the data we want is always in a table, which we can find with the `table` tag. There are some other tables, such as the footers and page navigation that we don't want. Again from inspection, it looks like the tables we do want always have `class=wikitable`.\n",
    "4. The tables are then split into rows. A row either consists of headers (`th` table header tag) or data (`td` table data tag). We pull this, put it in a data frame if we actually get data, and skip if we get an error putting it into the data frame.\n",
    "\n",
    "When we first wrote the `fetch_movie_data` function below, we did not have the `drop_colspan` feature Or the 1919 correction. When it was ran on all time, there were a few problems:\n",
    "1. Starting in 2014, the movie lists have an \"Opening\" data column, whose entries span multiple rows. Even though our script didn't throw any errors for 2015 the data gathered is wrong: actors are listed as movie titles, titles are listed as opening dates, and so on. These cells have tags with colspan attributes, which are larger than 1. To fix this we added the `drop_colspan` flag, which does not include columns that span multiple cells.\n",
    "2. There is a \"typo\" on the page for 1919. The very last row of the table has an extra cell. Not sure what the best way to deal with this one edge case is. I ended up counting the number of columns read, and only take that many cells for the data rows.\n",
    "\n",
    "There are definitely better ways to handle these problems. In particular, my solution assumes the multi-columns are all up front in the table, and that the multicolumn and the 1919 typo don't coincide on any page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def fetch_movie_data(url, drop_colspan=True):\n",
    "    # Get the response from GET request\n",
    "    response = requests.get(url)  \n",
    "    # Throw error if API call has an error\n",
    "    if response.status_code != 200:\n",
    "        raise requests.HTTPError(\n",
    "            f'Couldn\\'t call API. Error {response.status_code}.'\n",
    "        )\n",
    "  \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Get all tables on the site where the class is wikitable\n",
    "    # This prevents it from including layout tabes, like the wikipedia footer etc\n",
    "    tables = soup.find_all('table', {'class': 'wikitable'})\n",
    "\n",
    "    fetched = []\n",
    "    for table in tables:\n",
    "        # Get all table rows (tr) tag\n",
    "        rows = table.find_all('tr')\n",
    "        # Assuming first row is headers, look for table header (th) tags\n",
    "        if drop_colspan:\n",
    "            columns = []; drop = 0\n",
    "            for col in rows[0].find_all('th'):\n",
    "                # If has an attribute for colspan > 1, don't include\n",
    "                try:\n",
    "                    int(col.attrs['colspan']) > 1\n",
    "                    drop += int(col.attrs['colspan'])\n",
    "                except:\n",
    "                    columns.append(col.text.strip())\n",
    "        else:\n",
    "            columns = [x.text.strip() for x in rows[0].find_all('th')]\n",
    "        \n",
    "        # Assuming remaining rows are data, look for table data (td) tags\n",
    "        data = [[x.text.strip() for x in row.find_all('td')] for row in rows[1:]]\n",
    "        \n",
    "        if drop_colspan:\n",
    "            # Assuming the multicols are all in front\n",
    "            if drop > 0:\n",
    "                data = [row[-len(columns):] for row in data]\n",
    "        # Deal with error in the 1919 Wikipedia page...\n",
    "        # data = [row[:len(columns)] for row in data]\n",
    "        # Make sure we got data. \n",
    "        # If for whatever reason there is an error, don't include\n",
    "        if data and columns:\n",
    "            try:\n",
    "                df = pd.DataFrame(data, columns=columns)\n",
    "            except:\n",
    "                continue\n",
    "            fetched.append(df)\n",
    "    return pd.concat(fetched, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_1994 = fetch_movie_data('https://en.wikipedia.org/wiki/List_of_American_films_of_1952')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Director</th>\n",
       "      <th>Cast</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Notes/Studio</th>\n",
       "      <th>Notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Yankee Buccaneer</td>\n",
       "      <td>Frederick de Cordova</td>\n",
       "      <td>Jeff Chandler, Scott Brady, Suzan Ball</td>\n",
       "      <td>Adventure</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Universal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>You for Me</td>\n",
       "      <td>Don Weis</td>\n",
       "      <td>Jane Greer, Peter Lawford</td>\n",
       "      <td>Romance</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MGM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Young Man with Ideas</td>\n",
       "      <td>Mitchell Leisen</td>\n",
       "      <td>Glenn Ford, Ruth Roman</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MGM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Yukon Gold</td>\n",
       "      <td>Frank McDonald</td>\n",
       "      <td>Kirby Grant, Martha Hyer</td>\n",
       "      <td>Western</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Monogram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Zombies of the Stratosphere</td>\n",
       "      <td>Fred C. Brannon</td>\n",
       "      <td></td>\n",
       "      <td>Serial</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Republic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Title              Director  \\\n",
       "91             Yankee Buccaneer  Frederick de Cordova   \n",
       "92                   You for Me              Don Weis   \n",
       "93         Young Man with Ideas       Mitchell Leisen   \n",
       "94                   Yukon Gold        Frank McDonald   \n",
       "95  Zombies of the Stratosphere       Fred C. Brannon   \n",
       "\n",
       "                                      Cast      Genre Notes/Studio      Notes  \n",
       "91  Jeff Chandler, Scott Brady, Suzan Ball  Adventure          NaN  Universal  \n",
       "92               Jane Greer, Peter Lawford    Romance          NaN        MGM  \n",
       "93                  Glenn Ford, Ruth Roman     Comedy          NaN        MGM  \n",
       "94                Kirby Grant, Martha Hyer    Western          NaN   Monogram  \n",
       "95                                             Serial          NaN   Republic  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_1994.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've gotten things to appear to work for one year, let's creep that net. The above tutorial suggests using a timer to prevent us from sending too many requests to Wikipedia and getting blocked. I'm expecting the `fetch_movie_data` to fail sometimes, so let's collect our data in a dict. That way we can know exactly what years failed and what years we have data for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each year from 1900 to 2017\n",
    "movie_data = {}\n",
    "for year in range(1900, 2018):\n",
    "    url = f'https://en.wikipedia.org/wiki/List_of_American_films_of_{year}'\n",
    "    try:\n",
    "        data = fetch_movie_data(url)\n",
    "        movie_data[year] = data\n",
    "    except:\n",
    "        print(f'Something went wrong fetching data for {year}')\n",
    "        pass\n",
    "    # Pause for 2 sec to not overwhelm Wikipedia\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This took maybe 10 minutes to run. \n",
    "\n",
    "We really only want the title and the year, so let's get that. We could also get the cast and genre from here, but I feel like the OMDb versions of this will be more consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year, movies in movie_data.items():\n",
    "    movies['Year'] = year\n",
    "movies_full = pd.concat([movies[['Title', 'Year']] for movies in movie_data.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(movies_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have ~26K movies, which would take about 26 days to get IMDb data for using the free OMDb API. Maybe we should kick them a buck to do this faster.\n",
    "\n",
    "### 1.3. Combining Everything\n",
    "\n",
    "Note to future self: When running this we got fewer results when requesting full synopsis. The API only return results with a full synopsis if requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_teh_jsons = []\n",
    "for row in movies_full.itertuples(index=False, name=None):\n",
    "    try:\n",
    "        response_json = get_movie_data(*row)\n",
    "    except:\n",
    "        # If can't find with year, don't use\n",
    "        try:\n",
    "            response_json = get_movie_data(row[0])\n",
    "        except:\n",
    "            # Continue if no data found\n",
    "            continue\n",
    "        if response_json['Plot'] != 'N/A':\n",
    "            all_teh_jsons.append(response_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
